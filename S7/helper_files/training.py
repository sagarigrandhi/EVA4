# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUk2Ksz5o50WvwUc-PFQ0TJu3AkDPe_P
"""

from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F

def train(model, 
          device, 
          train_loader, 
          optimizer, 
          epoch,
          EPOCHS,
          train_loss, 
          train_accuracy,
          criterion = nn.CrossEntropyLoss()):
  '''
  Function to train the defined model on the training set.

  :param model: (class), defined CNN model
  :param device: (class), either CUDA or CPU
  :param train_loader: (class), dataloader for the training data
  :param optimizer: (class), optimizer for the training
  :param epoch: (int), specific epoch number
  :param EPOCHS: (int), total number of epochs
  :param train_loss: (list), list of losses accumulated per epoch during training
  :param train_accuracy: (list), list of accuracies accumulated per epoch during training
  :param criterion: (function), objective function to minimize
  '''
  model.train() 
  pbar = tqdm(train_loader)
  running_loss = 0.0
  correct = 0
  total = 0  
  
  for batch_idx, (data, target) in enumerate(pbar):
    # Get the samples
    inputs, labels = data.to(device), target.to(device)

    # Zero the parameter gradients
    optimizer.zero_grad()

    # Forward pass
    output = model(inputs)

    # Calculate loss
    loss = criterion(output, labels) 
    running_loss += loss.item()
        
    # Backpropagation
    loss.backward()
    optimizer.step()

    # Predict and update pbar-tqdm
    _, pred = torch.max(output.data, 1)
    correct += (pred == labels).sum().item()
    total += labels.size(0)
    pbar.set_description(desc=f'Epoch : {epoch+1}/{EPOCHS}, Batch_id : {batch_idx}, Training Loss : {loss.item():.4f},  Training Accuracy : {100 * (correct/total):.2f}%')

  # Train loss and accuracy 
  train_loss.append(running_loss/len(train_loader))
  train_accuracy.append(100 * (correct / total))