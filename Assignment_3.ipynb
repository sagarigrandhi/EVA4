{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nisag/EVA4/blob/master/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "82b0925c-b06a-40ed-8ccb-22098dca30cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "6ecc27f8-44fb-4bfd-8a4d-3c86ba3b37b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3376, 0.6734, 0.0745, 0.2112],\n",
              "         [0.0814, 0.8308, 0.7487, 0.6271],\n",
              "         [0.9845, 0.0485, 0.6864, 0.7887]],\n",
              "\n",
              "        [[0.5884, 0.9727, 0.4376, 0.7048],\n",
              "         [0.6403, 0.3292, 0.0975, 0.3133],\n",
              "         [0.7179, 0.2060, 0.5643, 0.8551]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "21416373-fab7-4895-fe1b-52c2b8107533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[2.0305e-01, 9.2029e-01, 1.9110e-01],\n",
              "          [4.0754e-01, 5.9344e-01, 8.2199e-01]],\n",
              "\n",
              "         [[8.4118e-01, 1.4222e-01, 2.5943e-01],\n",
              "          [1.2357e-01, 3.4148e-01, 5.1802e-01]]],\n",
              "\n",
              "\n",
              "        [[[8.8669e-01, 1.6954e-01, 3.1861e-01],\n",
              "          [5.5528e-04, 8.3755e-01, 7.2012e-01]],\n",
              "\n",
              "         [[7.3024e-02, 5.1420e-01, 5.0282e-01],\n",
              "          [4.7896e-01, 9.2099e-01, 8.3083e-01]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below? Ans: 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "b98a76e6-8103-4d6b-a561-12ca2fae8dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "torch.rand(1, 1, 1, 1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.4799]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQnbJ_jcPWzW",
        "colab_type": "code",
        "outputId": "d07813c0-e51c-485a-f484-4330922e25a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Q1\n",
        "a = torch.rand(1, 1, 1, 1)\n",
        "print(a.dim())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        ".\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contain - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "50949224-02af-4101-895e-4804b87a1915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkhHpuM_V293",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0c94724c-940f-4f45-bb56-6dd36a9c9049"
      },
      "source": [
        "# Memory behavior\n",
        "np_array = np.array([1,2])\n",
        "print(np_array.__array_interface__['data'][0])\n",
        "\n",
        "tensor_list = torch.tensor([1,2])\n",
        "print(tensor_list.data_ptr())\n",
        "\n",
        "tensor_array1 = torch.tensor(np_array)\n",
        "print(tensor_array1.data_ptr())\n",
        "\n",
        "tensor_array2 = torch.from_numpy(np_array)\n",
        "print(tensor_array2.data_ptr()) # shares np_array address\n",
        "\n",
        "array_tensor1 = tensor_array1.numpy()\n",
        "print(array_tensor1.__array_interface__['data'][0]) # shares the tensor_array1 address\n",
        "\n",
        "array_tensor2 = tensor_array2.numpy()\n",
        "print(array_tensor2.__array_interface__['data'][0]) # shares the tensor_array2 address which is sharing np_array address"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96430016\n",
            "96326592\n",
            "69744128\n",
            "96430016\n",
            "69744128\n",
            "96430016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that **pytorch avoids memory copying if it can.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "9a96f34c-8d04-4b36-a369-e5783445c241",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2) # changes to numpy array changes the tensor."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "Ans: Not attempted \n",
        "(Our model will not train and get stuck at 50% accuracy.)\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "Ans: Not attempted \n",
        "\n",
        "(The way we have implemented the strategy, we will end up adding black blocks on images while changing the original image. After few operations, whole image will just be black.Then network would see just black images for dogs and cats, and thereby failing in recognizing either, getting stuck at 50% accuracy.) \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "1b56b36c-c9a7-48e5-cc7e-4d55034a8a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy() # created from  torch.tensor(np_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "\n",
        "tensor_from_array[0] = 11  # changes to tensor changes the numpy array.\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch supports different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "ee821dcb-b579-4ac7-8592-66e8cc425913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values?  Ans: 1.0s \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9wKttDoGtV8",
        "colab_type": "code",
        "outputId": "cadc7136-de6f-4f60-c32c-e696bebb6d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Q4\n",
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "print(tensor.data_ptr())\n",
        "num = tensor.numpy() \n",
        "print(num.__array_interface__['data'][0]) \n",
        "num = num * 0.5 \n",
        "print(num.__array_interface__['data'][0]) # it's not a pointer linking back\n",
        "print(tensor)\n",
        "print(num)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69738560\n",
            "69738560\n",
            "91484400\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n",
            "[[0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U8VKs2kSQVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "691353d8-caed-4327-d37c-facb96c8e5ec"
      },
      "source": [
        "# Similar example\n",
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "print(tensor.data_ptr())\n",
        "num = tensor.numpy() \n",
        "print(num.__array_interface__['data'][0]) \n",
        "num = num + 0.5\n",
        "print(num.__array_interface__['data'][0]) \n",
        "print(tensor)\n",
        "print(num)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69743872\n",
            "69743872\n",
            "42984768\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n",
            "[[1.5 1.5]\n",
            " [1.5 1.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*0.5` will the original tensor have 1.0s or 0.5s as its element values? Ans: 0.5s\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iN_WcGEIPMp",
        "colab_type": "code",
        "outputId": "eb092768-afc6-4d8f-c606-eb3ee7156597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Q5 # A slice of an array is a view into the same data, so modifying it will modify the original array.\n",
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "print(tensor.data_ptr())\n",
        "num = tensor.numpy()\n",
        "print(num.__array_interface__['data'][0]) \n",
        "num[:] = num * 0.5 # we have made an array as a pointer # get a slice of the whole\n",
        "print(num.__array_interface__['data'][0]) \n",
        "print(tensor) \n",
        "print(num)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69743872\n",
            "69743872\n",
            "69743872\n",
            "tensor([[0.5000, 0.5000],\n",
            "        [0.5000, 0.5000]], dtype=torch.float16)\n",
            "[[0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKsHDzHaSTpK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "106daaae-ffdc-4f31-aa12-f635db70e63f"
      },
      "source": [
        "# Similar example\n",
        "tensor = torch.ones(2, 2, dtype=torch.float16)\n",
        "print(tensor.data_ptr())\n",
        "num = tensor.numpy() \n",
        "print(num.__array_interface__['data'][0]) \n",
        "num += 0.5\n",
        "print(num.__array_interface__['data'][0]) \n",
        "print(tensor)\n",
        "print(num)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "69738560\n",
            "69738560\n",
            "69738560\n",
            "tensor([[1.5000, 1.5000],\n",
            "        [1.5000, 1.5000]], dtype=torch.float16)\n",
            "[[1.5 1.5]\n",
            " [1.5 1.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "2082df07-0961-47b7-c258-ba060dd66c45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? Ans: No\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSdr6OfTI4aM",
        "colab_type": "code",
        "outputId": "6921505f-a442-40db-9f73-5c6a7e9117fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Q6\n",
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "cat_0 = torch.cat((a,b),dim=0)\n",
        "cat_1 = torch.cat((a,b),dim=1)\n",
        "print(cat_0)\n",
        "print(cat_1.T) # can check shapes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tensor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "d300ce4c-bcbf-4f85-d164-21451d567dc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "print(a[indices])\n",
        "\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange between 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices.\n",
        "\n",
        "Ans: False (this is the right answer)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub3Hbe0OLzN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q7 \n",
        "a = torch.arange(start=0, end=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byj8WXm0KoA2",
        "colab_type": "code",
        "outputId": "3358e16f-ae10-4548-997a-e8c66b874853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "indices = torch.arange(start = 0, end = 10) % 5\n",
        "b = a[indices[5:]]\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1wTO1N4LUJS",
        "colab_type": "code",
        "outputId": "a390871e-2a26-4670-e469-e8c3aaf15b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "indices = torch.arange(start = 1,end = 11) % 5\n",
        "b = a[indices[5:]]\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "f41777d9-7f8c-4287-9ba6-7d7cd5d10544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 2])\n",
        "print(tensor)\n",
        "tensor[rows]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9252, 0.0698, 0.1529],\n",
            "        [0.6992, 0.8330, 0.7271],\n",
            "        [0.3938, 0.9105, 0.4711],\n",
            "        [0.8296, 0.5002, 0.0853],\n",
            "        [0.8008, 0.7625, 0.8297]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9252, 0.0698, 0.1529],\n",
              "        [0.3938, 0.9105, 0.4711]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ? Ans: False\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM7f0DOlNMKX",
        "colab_type": "code",
        "outputId": "7ec08875-0c71-4163-df23-815deac4f856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Q8\n",
        "tensor = torch.rand((6,5))\n",
        "new_rows = torch.tensor([0,2,4])\n",
        "tensor[new_rows].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_F__xNRNhTj",
        "colab_type": "code",
        "outputId": "b1758cae-8ba3-4cc1-a281-1c40799575d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tensor = torch.rand((6,5))\n",
        "tensor_trans = torch.transpose(tensor,0,1)\n",
        "newer_rows = torch.tensor([0,2,4])\n",
        "tensor_trans[newer_rows].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "0ab54506-8202-4fb7-fa96-4a4e3fdb67ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "assert tensor.data_ptr() == view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  74640640\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 74640640\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 74640640\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "e5f59562-f8a9-4fe7-8498-30d6dc87d98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEyAKwHVR1T0",
        "colab_type": "code",
        "outputId": "774c2b36-273b-4be0-fe51-2b2e5235280a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Q2\n",
        "a = torch.tensor([[1,2,3],[1,2,3]])\n",
        "a = a.reshape((-1,2))\n",
        "print(a.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh21ULmMSfX2",
        "colab_type": "code",
        "outputId": "c060d39f-9ccd-4203-95d2-7c1e1b79c2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "b = torch.tensor([[2,1]])\n",
        "b = b.view(2,-1)\n",
        "print(b.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DeQiFoxSp27",
        "colab_type": "code",
        "outputId": "b5482991-554e-4ea7-b4dc-67c60e195fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(int(torch.sum(torch.mm(a,b))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "c6dcd262-a31d-4c61-ebb8-e23fecdb9c10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMeUlEQVR4nO3dX6hl5XnH8e+vGr1o4r9KJzIO1Ya5saATO4iQWCzSxngz5kb0Ig4iTC4UEmgvJulFvAmkhaQgtMIExRFSrZAE58L+sUNEvNA4jXYcterUaJ1hnKG1ZCwBU83Ti7NMdsw5nj9777PPfs73A4u99rvX2ut9z3nOj7XW2WvtVBWSpF5+a9YdkCRNnuEuSQ0Z7pLUkOEuSQ0Z7pLUkOEuSQ1NLdyTXJ/k5SRHk+yd1nak9WRda15kGp9zT3IG8ArwJ8Ax4Bnglqp6ceIbk9aJda15Mq0996uAo1X1WlX9HHgI2DWlbUnrxbrW3JhWuG8F3hx5fmxok+aZda25ceasNpxkD7BnePqHs+qHNoeqynpty9rWelqqtqcV7seBbSPPLx7aRju0D9gHkMQb3GgeLFvXYG1rY5jWaZlngO1JLk1yFnAzcGBK25LWi3WtuTGVPfeqei/JncA/AWcA91XVC9PYlrRerGvNk6l8FHLVnfDQVVO2nufcR1nbmralatsrVCWpIcNdkhoy3CWpIcNdkhqa2UVMa3H2ufDJHbPuhTaKt56Fd0/PuheTce6557Jjh8WtBc8++yynT49X3HMV7p+8Am57fNa90EZx3zXwn0/OuheTccUVV/D444/PuhvaIK655hqefHK84va0jCQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1ZLhLUkOGuyQ1NNb93JO8DrwDvA+8V1U7k1wA/D1wCfA6cFNV/c943ZTWl7WteTeJPfc/rqodVbVzeL4XOFhV24GDw3NpHlnbmlvTOC2zC9g/zO8HbpzCNqRZsLY1N8YN9wL+Ocm/JtkztG2pqhPD/FvAljG3Ic2Cta25Nu53qH62qo4n+V3gsST/PvpiVVWSWmzF4Q9mz2KvSRuAta25Ntaee1UdHx5PAT8ArgJOJrkIYHg8tcS6+6pq58j5TGnDsLY179Yc7kl+O8knPpgH/hQ4AhwAdg+L7QYeGbeT0nqyttXBOKdltgA/SPLB+/xdVf1jkmeAh5PcDrwB3DR+N6V1ZW1r7q053KvqNeCKRdr/G7hunE5Js2RtqwOvUJWkhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhgx3SWrIcJekhpYN9yT3JTmV5MhI2wVJHkvy6vB4/tCeJHcnOZrkcJIrp9l5aRzWtjpbyZ77/cD1H2rbCxysqu3AweE5wOeB7cO0B7hnMt2UpuJ+rG01tWy4V9UTwNsfat4F7B/m9wM3jrQ/UAueAs5LctGkOitNkrWtztZ6zn1LVZ0Y5t8CtgzzW4E3R5Y7NrT9hiR7khxKcmiNfZCmwdpWC2eO+wZVVUlqDevtA/YBrGV9adqsbc2zte65n/zgkHR4PDW0Hwe2jSx38dAmzQtrWy2sNdwPALuH+d3AIyPttw6fLLga+OnIIa40D6xttbDsaZkkDwLXAhcmOQZ8Hfgm8HCS24E3gJuGxR8FbgCOAj8DbptCn6WJsLbV2bLhXlW3LPHSdYssW8Ad43ZKWg/WtjrzClVJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGDHdJashwl6SGlg33JPclOZXkyEjbXUmOJ3lumG4Yee2rSY4meTnJ56bVcWlc1rY6W8me+/3A9Yu0/3VV7RimRwGSXAbcDPzBsM7fJjljUp2VJux+rG01tWy4V9UTwNsrfL9dwENV9W5V/QQ4Clw1Rv+kqbG21dk459zvTHJ4OLQ9f2jbCrw5ssyxoU2aJ9a25t5aw/0e4FPADuAE8K3VvkGSPUkOJTm0xj5I02Btq4U1hXtVnayq96vqF8B3+NXh6XFg28iiFw9ti73HvqraWVU719IHaRqsbXWxpnBPctHI0y8AH3za4ABwc5Kzk1wKbAd+NF4XpfVjbauLM5dbIMmDwLXAhUmOAV8Hrk2yAyjgdeBLAFX1QpKHgReB94A7qur96XRdGo+1rc6WDfequmWR5ns/YvlvAN8Yp1PSerC21ZlXqEpSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ4a7JDVkuEtSQ8uGe5JtSX6Y5MUkLyT58tB+QZLHkrw6PJ4/tCfJ3UmOJjmc5MppD0JaC2tbna1kz/094M+q6jLgauCOJJcBe4GDVbUdODg8B/g8sH2Y9gD3TLzX0mRY22pr2XCvqhNV9eNh/h3gJWArsAvYPyy2H7hxmN8FPFALngLOS3LRxHsujcnaVmerOuee5BLg08DTwJaqOjG89BawZZjfCrw5stqxoU3asKxtdXPmShdM8nHge8BXqup0kl++VlWVpFaz4SR7WDi0lWbK2lZHK9pzT/IxFor/u1X1/aH55AeHpMPjqaH9OLBtZPWLh7ZfU1X7qmpnVe1ca+elcVnb6moln5YJcC/wUlV9e+SlA8DuYX438MhI+63DJwuuBn46cogrbRjWtjpbyWmZzwBfBJ5P8tzQ9jXgm8DDSW4H3gBuGl57FLgBOAr8DLhtoj2WJsfaVlvLhntVPQlkiZevW2T5Au4Ys1/S1Fnb6swrVCWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpIcNdkhoy3CWpoSx8ucyMO7HCb5c/+xzYcvm0e6N5cfIwvHt6ZctW1VLfuDRVK63tc845h8svt7i14PDhw5w+vbLiXqq25yrcpbXa6OEurdVSte1pGUlqyHCXpIaWDfck25L8MMmLSV5I8uWh/a4kx5M8N0w3jKzz1SRHk7yc5HPTHIC0Vta2Wquqj5yAi4Arh/lPAK8AlwF3AX++yPKXAf8GnA1cCvwHcMYy2ygnp2lO1rZT12mp2lt2z72qTlTVj4f5d4CXgK0fscou4KGqereqfgIcBa5abjvSerO21dmqzrknuQT4NPD00HRnksNJ7kty/tC2FXhzZLVjfPQfjDRz1ra6WXG4J/k48D3gK1V1GrgH+BSwAzgBfGs1G06yJ8mhJIdWs540ada2OlpRuCf5GAvF/92q+j5AVZ2sqver6hfAd/jV4elxYNvI6hcPbb+mqvZV1c6q2jnOAKRxWNvqaiWflglwL/BSVX17pP2ikcW+ABwZ5g8ANyc5O8mlwHbgR5PrsjQZ1rY6O3MFy3wG+CLwfJLnhravAbck2cHCf2xfB74EUFUvJHkYeBF4D7ijqt5fZhv/C7y8+u7PrQuB/5p1J9bJRhjr7y3Rbm1P3kb4fa+XjTDWpWp7w9x+4NBmOoTdTOPdTGNdzGYb/2Ya70Yfq1eoSlJDhrskNbRRwn3frDuwzjbTeDfTWBez2ca/mca7oce6Ic65S5Ima6PsuUuSJmjm4Z7k+uEOe0eT7J11fyZhuGT9VJIjI20XJHksyavD4/lDe5LcPYz/cJIrZ9fz1fuIOyu2HO9qdKtt63rOxrvcXSGnOQFnsHBnvd8HzmLhjnuXzbJPExrXHwFXAkdG2v4K2DvM7wX+cpi/AfgHIMDVwNOz7v8qx7rUnRVbjncVP5d2tW1dz1ddz3rP/SrgaFW9VlU/Bx5i4c57c62qngDe/lDzLmD/ML8fuHGk/YFa8BRw3oeukNzQauk7K7Yc7yq0q23rer7qetbhvpnusrelqk4M828BW4b5Nj+DD91Zsf14l7FZxtn+9zyvdT3rcN+UauE4rtXHlBa5s+IvdRyvflPH3/M81/Wsw31Fd9lr4uQHh2nD46mhfe5/BovdWZHG412hzTLOtr/nea/rWYf7M8D2JJcmOQu4mYU773V0ANg9zO8GHhlpv3X4b/vVwE9HDvs2vKXurEjT8a7CZqntlr/nFnU96//osvBf5ldY+GTBX8y6PxMa04MsfMnD/7Fw7u124HeAg8CrwL8AFwzLBvibYfzPAztn3f9VjvWzLByaHgaeG6Ybuo53lT+bVrVtXc9XXXuFqiQ1NOvTMpKkKTDcJakhw12SGjLcJakhw12SGjLcJakhw12SGjLcJamh/wcL9cB71K+IKAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMkElEQVR4nO3dXYxc5X3H8e+vduCiCW9BdSzbKm7kG1fCG7pCSIkrKtSG+MbkBsFFsBCScwFSIqUXTnoRbiKllZJKSC2SIyyMlEItJRG+oC/UCkK+gODGdDFQYEug2DK2WiqWKhKpyb8Xe5xMnF3vy8zszDz7/UhHc+aZc+Y8z+5/fzrn7JwzqSokSW35nVF3QJI0eIa7JDXIcJekBhnuktQgw12SGmS4S1KDhhbuSW5P8lqS2SQHhrUdaS1Z15oUGcbn3JNsAF4H/hQ4DbwA3F1Vrwx8Y9Iasa41SYa1534zMFtVb1bVL4AngL1D2pa0VqxrTYxhhfsW4J2e56e7NmmSWdeaGBtHteEk+4H93dM/GlU/tD5UVdZqW9a21tJitT2scD8DbOt5vrVr6+3QQeAgQBJvcKNJsGRdg7Wt8TCs0zIvADuSbE9yBXAXcHRI25LWinWtiTGUPfequpDkAeCfgA3Aoap6eRjbktaKda1JMpSPQq64Ex66asjW8px7L2tbw7ZYbXuFqiQ1yHCXpAYZ7pLUIMNdkho0souYVuPqq69kaupTo+6GxsTJk+8yN/fhqLsxEFdffTVTU1Oj7obGxMmTJ5mbm+vrPSYq3Hft+hTPPHPvqLuhMbF79yGOH//PUXdjIHbt2sUzzzwz6m5oTOzevZvjx4/39R6elpGkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBfd3PPclbwAfAR8CFqppOch3w98ANwFvAnVX1P/11U1pb1rYm3SD23P+kqqaqarp7fgA4VlU7gGPdc2kSWduaWMM4LbMXONzNHwbuGMI2pFGwtjUx+g33Av45yb8m2d+1baqqs938u8CmPrchjYK1rYnW73eofq6qziT5PeDpJP/e+2JVVZJaaMXuD2b/Qq9JY8Da1kTra8+9qs50j+eBHwE3A+eSbAboHs8vsu7BqpruOZ8pjQ1rW5Nu1eGe5HeTfOLiPPBnwCngKLCvW2wf8GS/nZTWkrWtFvRzWmYT8KMkF9/n76rqH5O8ABxJch/wNnBn/92U1pS1rYm36nCvqjeBXQu0/zdwWz+dkkbJ2lYLvEJVkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUFLhnuSQ0nOJznV03ZdkqeTvNE9Xtu1J8lDSWaTzCS5aZidl/phbatly9lzfxS4/ZK2A8CxqtoBHOueA3wB2NFN+4GHB9NNaSgexdpWo5YM96p6Fnjvkua9wOFu/jBwR0/7YzXvOeCaJJsH1VlpkKxttWy159w3VdXZbv5dYFM3vwV4p2e5013bb0myP8mJJCdW2QdpGKxtNWFjv29QVZWkVrHeQeAgwGrWl4bN2tYkW+2e+7mLh6Td4/mu/QywrWe5rV2bNCmsbTVhteF+FNjXze8Dnuxpv6f7ZMEtwPs9h7jSJLC21YQlT8skeRy4Fbg+yWngm8C3gSNJ7gPeBu7sFn8K2APMAj8H7h1Cn6WBsLbVsiXDvaruXuSl2xZYtoD7++2UtBasbbXMK1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktSgJcM9yaEk55Oc6ml7MMmZJC92056e176eZDbJa0k+P6yOS/2yttWy5ey5PwrcvkD7X1fVVDc9BZBkJ3AX8IfdOn+bZMOgOisN2KNY22rUkuFeVc8C7y3z/fYCT1TVh1X1M2AWuLmP/klDY22rZf2cc38gyUx3aHtt17YFeKdnmdNdmzRJrG1NvNWG+8PAp4Ep4CzwnZW+QZL9SU4kObHKPkjDYG2rCasK96o6V1UfVdUvge/x68PTM8C2nkW3dm0LvcfBqpququnV9EEaBmtbrVhVuCfZ3PP0i8DFTxscBe5KcmWS7cAO4Cf9dVFaO9a2WrFxqQWSPA7cClyf5DTwTeDWJFNAAW8BXwaoqpeTHAFeAS4A91fVR8PputQfa1stWzLcq+ruBZofuczy3wK+1U+npLVgbatlXqEqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KAlwz3JtiQ/TvJKkpeTfKVrvy7J00ne6B6v7dqT5KEks0lmktw07EFIq2Ftq2XL2XO/AHytqnYCtwD3J9kJHACOVdUO4Fj3HOALwI5u2g88PPBeS4NhbatZS4Z7VZ2tqp928x8ArwJbgL3A4W6xw8Ad3fxe4LGa9xxwTZLNA++51CdrWy1b0Tn3JDcAnwGeBzZV1dnupXeBTd38FuCdntVOd23S2LK21ZqNy10wyceBHwBfraq5JL96raoqSa1kw0n2M39oK42Uta0WLWvPPcnHmC/+71fVD7vmcxcPSbvH8137GWBbz+pbu7bfUFUHq2q6qqZX23mpX9a2WrWcT8sEeAR4taq+2/PSUWBfN78PeLKn/Z7ukwW3AO/3HOJKY8PaVsuWc1rms8CXgJeSvNi1fQP4NnAkyX3A28Cd3WtPAXuAWeDnwL0D7bE0ONa2mrVkuFfVcSCLvHzbAssXcH+f/ZKGztpWy7xCVZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhqU+S+XGXEnlvnt8ldddSU33rhp2N3RhJiZOcfc3IfLWraqFvvGpaFafm1fxY033jjs7mhCzMzMMDc3t6xlF6vtiQp3abXGPdyl1Vqstj0tI0kNMtwlqUFLhnuSbUl+nOSVJC8n+UrX/mCSM0le7KY9Pet8PclskteSfH6YA5BWy9pW06rqshOwGbipm/8E8DqwE3gQ+PMFlt8J/BtwJbAd+A9gwxLbKCenYU7WtlOr02K1t+See1WdraqfdvMfAK8CWy6zyl7giar6sKp+BswCNy+1HWmtWdtq2YrOuSe5AfgM8HzX9ECSmSSHklzbtW0B3ulZ7TSX/4ORRs7aVmuWHe5JPg78APhqVc0BDwOfBqaAs8B3VrLhJPuTnEhyYiXrSYNmbatFywr3JB9jvvi/X1U/BKiqc1X1UVX9Evgevz48PQNs61l9a9f2G6rqYFVNV9V0PwOQ+mFtq1XL+bRMgEeAV6vquz3tm3sW+yJwqps/CtyV5Mok24EdwE8G12VpMKxttWzjMpb5LPAl4KUkL3Zt3wDuTjLF/H9s3wK+DFBVLyc5ArwCXADur6qPltjG/wKvrbz7E+t64L9G3Yk1Mg5j/f1F2q3twRuH3/daGYexLlbbY3P7gRPr6RB2PY13PY11Iett/OtpvOM+Vq9QlaQGGe6S1KBxCfeDo+7AGltP411PY13Iehv/ehrvWI91LM65S5IGa1z23CVJAzTycE9ye3eHvdkkB0bdn0HoLlk/n+RUT9t1SZ5O8kb3eG3XniQPdeOfSXLT6Hq+cpe5s2KT412J1mrbup6w8S51V8hhTsAG5u+s9wfAFczfcW/nKPs0oHH9MXATcKqn7a+AA938AeAvu/k9wD8AAW4Bnh91/1c41sXurNjkeFfwc2mutq3ryarrUe+53wzMVtWbVfUL4Anm77w30arqWeC9S5r3Aoe7+cPAHT3tj9W854BrLrlCcqzV4ndWbHK8K9BcbVvXk1XXow739XSXvU1Vdbabfxe4+E3fzfwMLrmzYvPjXcJ6GWfzv+dJretRh/u6VPPHcU19TGmBOyv+Sovj1W9r8fc8yXU96nBf1l32GnHu4mFa93i+a5/4n8FCd1ak4fEu03oZZ7O/50mv61GH+wvAjiTbk1wB3MX8nfdadBTY183vA57sab+n+2/7LcD7PYd9Y2+xOyvS6HhXYL3UdpO/5ybqetT/0WX+v8yvM//Jgr8YdX8GNKbHmf+Sh/9j/tzbfcAngWPAG8C/ANd1ywb4m278LwHTo+7/Csf6OeYPTWeAF7tpT6vjXeHPpqnatq4nq669QlWSGjTq0zKSpCEw3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatD/Ax0Jxmt3tlFaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python libraries work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CPp4scJBqQm",
        "colab_type": "code",
        "outputId": "d2a29da2-17a1-4a91-8d84-fb9c66e1582e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# this will prompt for authorization\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "# dataset file path\n",
        "filepath = \"gdrive/My Drive/colab_folder/datasets/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "38fa5a9f-fb68-4128-bf57-d9ab1c92c336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "bgr_image = cv2.imread(filepath+'mars.jpg') \n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1] # instead of bgr_image[:,:,::-1] can use this \n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwVa7LOMDeIN",
        "colab_type": "code",
        "outputId": "22c646d8-bc93-4d9b-90f7-30d0fb5ada9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Q10\n",
        "print(bgr_image[:,:,0]) # blue channel (can take sum or mean to compare)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5 5 5 ... 5 5 5]\n",
            " [5 5 5 ... 5 5 5]\n",
            " [5 5 5 ... 5 5 5]\n",
            " ...\n",
            " [5 5 5 ... 5 5 5]\n",
            " [3 4 4 ... 6 6 6]\n",
            " [3 4 4 ... 6 6 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1RveM1gD5i7",
        "colab_type": "code",
        "outputId": "ee06b682-dee6-4f41-cb47-747dae6c0c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(bgr_image[:,:,2]) # red channel"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8 8 8 ... 8 8 8]\n",
            " [8 8 8 ... 8 8 8]\n",
            " [8 8 8 ... 8 8 8]\n",
            " ...\n",
            " [8 8 8 ... 8 8 8]\n",
            " [6 7 7 ... 9 9 9]\n",
            " [6 7 7 ... 9 9 9]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False? Ans: True\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "5c599865-3f67-419c-c454-d01251fb49b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0583, 0.2627, 0.9879, 0.5682, 0.0570],\n",
            "        [0.1553, 0.8325, 0.3682, 0.2947, 0.3625],\n",
            "        [0.2619, 0.6071, 0.2287, 0.8577, 0.7842]], requires_grad=True)\n",
            "tensor([[0.2916, 1.3134, 4.9394, 2.8411, 0.2852],\n",
            "        [0.7764, 4.1626, 1.8410, 1.4737, 1.8127],\n",
            "        [1.3095, 3.0357, 1.1433, 4.2885, 3.9212]], grad_fn=<MulBackward0>)\n",
            "tensor(33.4353, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above? Ans: Because that is what it should be based on how result is defined\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "outputId": "2a3b571f-df15-45ea-e5ec-b7aae26fcb9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "outputId": "8a47ef57-dd0f-4dae-842a-808b15d36113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "afbdc9d1-c19b-4c16-8861-a6894d61380d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 32089466.0\n",
            "1 31691962.0\n",
            "2 37101752.0\n",
            "3 41137828.0\n",
            "4 37660856.0\n",
            "5 25299734.0\n",
            "6 12915349.0\n",
            "7 5623759.0\n",
            "8 2624994.5\n",
            "9 1488066.375\n",
            "10 1023574.0625\n",
            "11 790927.625\n",
            "12 646592.75\n",
            "13 542675.4375\n",
            "14 461846.25\n",
            "15 396419.03125\n",
            "16 342462.125\n",
            "17 297422.8125\n",
            "18 259414.59375\n",
            "19 227195.9375\n",
            "20 199737.421875\n",
            "21 176210.015625\n",
            "22 155954.421875\n",
            "23 138419.96875\n",
            "24 123197.7734375\n",
            "25 109930.3515625\n",
            "26 98315.1171875\n",
            "27 88124.9140625\n",
            "28 79152.171875\n",
            "29 71231.7578125\n",
            "30 64217.5546875\n",
            "31 57988.73046875\n",
            "32 52443.52734375\n",
            "33 47506.82421875\n",
            "34 43097.1640625\n",
            "35 39149.2578125\n",
            "36 35607.3125\n",
            "37 32423.845703125\n",
            "38 29560.576171875\n",
            "39 26979.5390625\n",
            "40 24647.373046875\n",
            "41 22538.626953125\n",
            "42 20630.892578125\n",
            "43 18901.234375\n",
            "44 17330.95703125\n",
            "45 15903.7958984375\n",
            "46 14605.35546875\n",
            "47 13423.22265625\n",
            "48 12345.0712890625\n",
            "49 11361.74609375\n",
            "50 10463.4443359375\n",
            "51 9642.5625\n",
            "52 8891.7353515625\n",
            "53 8204.17578125\n",
            "54 7574.294921875\n",
            "55 6996.6103515625\n",
            "56 6466.32373046875\n",
            "57 5979.15771484375\n",
            "58 5531.513671875\n",
            "59 5119.74853515625\n",
            "60 4740.75439453125\n",
            "61 4391.9033203125\n",
            "62 4070.6904296875\n",
            "63 3774.410400390625\n",
            "64 3501.072021484375\n",
            "65 3248.908203125\n",
            "66 3016.2041015625\n",
            "67 2801.15869140625\n",
            "68 2602.384033203125\n",
            "69 2418.5361328125\n",
            "70 2248.578857421875\n",
            "71 2091.19189453125\n",
            "72 1945.4580078125\n",
            "73 1810.4847412109375\n",
            "74 1685.4730224609375\n",
            "75 1569.53173828125\n",
            "76 1461.993408203125\n",
            "77 1362.2261962890625\n",
            "78 1269.680908203125\n",
            "79 1183.7265625\n",
            "80 1103.8714599609375\n",
            "81 1029.7242431640625\n",
            "82 960.7831420898438\n",
            "83 896.701904296875\n",
            "84 837.1061401367188\n",
            "85 781.6541748046875\n",
            "86 730.0399169921875\n",
            "87 681.9962158203125\n",
            "88 637.3016967773438\n",
            "89 595.6456909179688\n",
            "90 556.8184204101562\n",
            "91 520.646240234375\n",
            "92 486.92425537109375\n",
            "93 455.50689697265625\n",
            "94 426.1884765625\n",
            "95 398.8332214355469\n",
            "96 373.30120849609375\n",
            "97 349.47003173828125\n",
            "98 327.23382568359375\n",
            "99 306.4696044921875\n",
            "100 287.0693359375\n",
            "101 268.94818115234375\n",
            "102 252.01678466796875\n",
            "103 236.18617248535156\n",
            "104 221.39834594726562\n",
            "105 207.5634765625\n",
            "106 194.62210083007812\n",
            "107 182.51763916015625\n",
            "108 171.1960906982422\n",
            "109 160.60018920898438\n",
            "110 150.6866455078125\n",
            "111 141.3994903564453\n",
            "112 132.70831298828125\n",
            "113 124.56612396240234\n",
            "114 116.94290924072266\n",
            "115 109.79988098144531\n",
            "116 103.10962677001953\n",
            "117 96.84070587158203\n",
            "118 90.96110534667969\n",
            "119 85.44904327392578\n",
            "120 80.28218078613281\n",
            "121 75.43746185302734\n",
            "122 70.8963851928711\n",
            "123 66.63510131835938\n",
            "124 62.637115478515625\n",
            "125 58.88587951660156\n",
            "126 55.365478515625\n",
            "127 52.06124496459961\n",
            "128 48.96059036254883\n",
            "129 46.050437927246094\n",
            "130 43.31578826904297\n",
            "131 40.74850845336914\n",
            "132 38.33755874633789\n",
            "133 36.07225036621094\n",
            "134 33.944217681884766\n",
            "135 31.946975708007812\n",
            "136 30.06972312927246\n",
            "137 28.304447174072266\n",
            "138 26.644527435302734\n",
            "139 25.085285186767578\n",
            "140 23.619003295898438\n",
            "141 22.24126434326172\n",
            "142 20.945396423339844\n",
            "143 19.727338790893555\n",
            "144 18.581172943115234\n",
            "145 17.502979278564453\n",
            "146 16.488719940185547\n",
            "147 15.534826278686523\n",
            "148 14.63770866394043\n",
            "149 13.793269157409668\n",
            "150 12.999387741088867\n",
            "151 12.250910758972168\n",
            "152 11.54762077331543\n",
            "153 10.88447380065918\n",
            "154 10.260496139526367\n",
            "155 9.672636985778809\n",
            "156 9.120630264282227\n",
            "157 8.599239349365234\n",
            "158 8.108769416809082\n",
            "159 7.646460056304932\n",
            "160 7.211261749267578\n",
            "161 6.801269054412842\n",
            "162 6.41471529006958\n",
            "163 6.051064491271973\n",
            "164 5.708620548248291\n",
            "165 5.385127067565918\n",
            "166 5.080749988555908\n",
            "167 4.793430805206299\n",
            "168 4.522907733917236\n",
            "169 4.267699241638184\n",
            "170 4.027247428894043\n",
            "171 3.8006227016448975\n",
            "172 3.5871567726135254\n",
            "173 3.385528802871704\n",
            "174 3.1954941749572754\n",
            "175 3.0160841941833496\n",
            "176 2.846958637237549\n",
            "177 2.6878983974456787\n",
            "178 2.537529468536377\n",
            "179 2.395753860473633\n",
            "180 2.261862277984619\n",
            "181 2.1357922554016113\n",
            "182 2.016735553741455\n",
            "183 1.9044404029846191\n",
            "184 1.798372745513916\n",
            "185 1.6983404159545898\n",
            "186 1.6040105819702148\n",
            "187 1.515106439590454\n",
            "188 1.430944561958313\n",
            "189 1.3514742851257324\n",
            "190 1.2767826318740845\n",
            "191 1.2060291767120361\n",
            "192 1.1392351388931274\n",
            "193 1.0763009786605835\n",
            "194 1.0168147087097168\n",
            "195 0.9608306884765625\n",
            "196 0.9077394604682922\n",
            "197 0.8577585220336914\n",
            "198 0.8104997873306274\n",
            "199 0.7659111618995667\n",
            "200 0.7236819267272949\n",
            "201 0.683910071849823\n",
            "202 0.646344780921936\n",
            "203 0.6109311580657959\n",
            "204 0.5774329900741577\n",
            "205 0.5456725358963013\n",
            "206 0.5158276557922363\n",
            "207 0.48753446340560913\n",
            "208 0.46089744567871094\n",
            "209 0.4355870187282562\n",
            "210 0.4118741452693939\n",
            "211 0.3893727660179138\n",
            "212 0.3680892288684845\n",
            "213 0.34804287552833557\n",
            "214 0.32902175188064575\n",
            "215 0.31103435158729553\n",
            "216 0.29409703612327576\n",
            "217 0.27806776762008667\n",
            "218 0.26290205121040344\n",
            "219 0.24859198927879333\n",
            "220 0.235122948884964\n",
            "221 0.22231946885585785\n",
            "222 0.21023496985435486\n",
            "223 0.19882725179195404\n",
            "224 0.18804728984832764\n",
            "225 0.17786160111427307\n",
            "226 0.168169304728508\n",
            "227 0.15904907882213593\n",
            "228 0.15047422051429749\n",
            "229 0.1423117071390152\n",
            "230 0.13459324836730957\n",
            "231 0.1273079514503479\n",
            "232 0.12045348435640335\n",
            "233 0.1139698401093483\n",
            "234 0.10780645161867142\n",
            "235 0.10193078964948654\n",
            "236 0.09645655751228333\n",
            "237 0.0912318155169487\n",
            "238 0.08632278442382812\n",
            "239 0.08165804296731949\n",
            "240 0.07724791765213013\n",
            "241 0.07310561090707779\n",
            "242 0.06915196776390076\n",
            "243 0.06544635444879532\n",
            "244 0.06193239986896515\n",
            "245 0.058579351752996445\n",
            "246 0.05541909858584404\n",
            "247 0.05246168375015259\n",
            "248 0.04963064566254616\n",
            "249 0.04696732014417648\n",
            "250 0.044464390724897385\n",
            "251 0.04208633676171303\n",
            "252 0.0398426353931427\n",
            "253 0.03769872710108757\n",
            "254 0.03567647561430931\n",
            "255 0.03376046195626259\n",
            "256 0.03194935619831085\n",
            "257 0.030239848420023918\n",
            "258 0.028646131977438927\n",
            "259 0.027114365249872208\n",
            "260 0.02567312866449356\n",
            "261 0.02428990975022316\n",
            "262 0.023009929805994034\n",
            "263 0.02179059013724327\n",
            "264 0.020619168877601624\n",
            "265 0.019532963633537292\n",
            "266 0.018504604697227478\n",
            "267 0.01751941628754139\n",
            "268 0.01659739576280117\n",
            "269 0.01571667566895485\n",
            "270 0.014893634244799614\n",
            "271 0.01410751324146986\n",
            "272 0.013356425799429417\n",
            "273 0.012654825113713741\n",
            "274 0.011990330182015896\n",
            "275 0.011363531462848186\n",
            "276 0.010766190476715565\n",
            "277 0.010206583887338638\n",
            "278 0.009686981327831745\n",
            "279 0.009176347404718399\n",
            "280 0.008701268583536148\n",
            "281 0.00824452843517065\n",
            "282 0.007818636484444141\n",
            "283 0.007406956050544977\n",
            "284 0.007025051862001419\n",
            "285 0.006662482395768166\n",
            "286 0.006323693785816431\n",
            "287 0.00600555632263422\n",
            "288 0.005698544904589653\n",
            "289 0.005410490557551384\n",
            "290 0.005139406304806471\n",
            "291 0.004879597574472427\n",
            "292 0.00463669840246439\n",
            "293 0.004404959734529257\n",
            "294 0.004186833743005991\n",
            "295 0.003981306217610836\n",
            "296 0.0037811973597854376\n",
            "297 0.003595139365643263\n",
            "298 0.003422170877456665\n",
            "299 0.0032530026510357857\n",
            "300 0.0031004094053059816\n",
            "301 0.0029494238551706076\n",
            "302 0.0028082956559956074\n",
            "303 0.0026726904325187206\n",
            "304 0.002543384674936533\n",
            "305 0.002424654783681035\n",
            "306 0.0023099915124475956\n",
            "307 0.0022042589262127876\n",
            "308 0.0021027603652328253\n",
            "309 0.0020054522901773453\n",
            "310 0.0019155384507030249\n",
            "311 0.001827130327001214\n",
            "312 0.0017446340061724186\n",
            "313 0.001666095806285739\n",
            "314 0.0015909613575786352\n",
            "315 0.0015227100811898708\n",
            "316 0.0014561369316652417\n",
            "317 0.0013925275998190045\n",
            "318 0.0013343562604859471\n",
            "319 0.0012783127604052424\n",
            "320 0.0012222232762724161\n",
            "321 0.001171415438875556\n",
            "322 0.0011226920178160071\n",
            "323 0.0010772815439850092\n",
            "324 0.001031561754643917\n",
            "325 0.0009894254617393017\n",
            "326 0.0009503162000328302\n",
            "327 0.0009120051981881261\n",
            "328 0.0008757855393923819\n",
            "329 0.0008415178162977099\n",
            "330 0.0008102411520667374\n",
            "331 0.0007764356560073793\n",
            "332 0.0007484469097107649\n",
            "333 0.0007192468037828803\n",
            "334 0.0006924096378497779\n",
            "335 0.0006671927403658628\n",
            "336 0.0006424812017939985\n",
            "337 0.0006188421975821257\n",
            "338 0.0005965256132185459\n",
            "339 0.0005741556524299085\n",
            "340 0.0005534234223887324\n",
            "341 0.0005344045930542052\n",
            "342 0.000515434134285897\n",
            "343 0.0004983714898116887\n",
            "344 0.00048265481018461287\n",
            "345 0.00046529213432222605\n",
            "346 0.0004506475233938545\n",
            "347 0.00043395848479121923\n",
            "348 0.0004200706316623837\n",
            "349 0.0004060349892824888\n",
            "350 0.00039213834679685533\n",
            "351 0.00037979535409249365\n",
            "352 0.0003681299858726561\n",
            "353 0.00035769116948358715\n",
            "354 0.0003452140081208199\n",
            "355 0.00033493462251499295\n",
            "356 0.00032398910843767226\n",
            "357 0.0003138938045594841\n",
            "358 0.00030494414386339486\n",
            "359 0.0002953540242742747\n",
            "360 0.00028632525936700404\n",
            "361 0.00027814743225462735\n",
            "362 0.00026948543381877244\n",
            "363 0.0002620525483507663\n",
            "364 0.0002544962044339627\n",
            "365 0.00024812170886434615\n",
            "366 0.00024067242338787764\n",
            "367 0.00023360547493211925\n",
            "368 0.00022752530639991164\n",
            "369 0.00022188315051607788\n",
            "370 0.00021561399626079947\n",
            "371 0.00020949976169504225\n",
            "372 0.00020370760466903448\n",
            "373 0.0001988468284253031\n",
            "374 0.00019365506886970252\n",
            "375 0.00018841461860574782\n",
            "376 0.0001833972637541592\n",
            "377 0.0001788739173207432\n",
            "378 0.00017470426973886788\n",
            "379 0.00016985838010441512\n",
            "380 0.00016559618234168738\n",
            "381 0.00016206740110646933\n",
            "382 0.00015786993026267737\n",
            "383 0.00015390419866889715\n",
            "384 0.0001504231768194586\n",
            "385 0.0001473967859055847\n",
            "386 0.00014348604599945247\n",
            "387 0.00014016228669788688\n",
            "388 0.00013667625898960978\n",
            "389 0.00013338391727302223\n",
            "390 0.000130674903630279\n",
            "391 0.00012726421118713915\n",
            "392 0.00012465575127862394\n",
            "393 0.0001217851386172697\n",
            "394 0.00011897602234967053\n",
            "395 0.00011649727821350098\n",
            "396 0.00011420507507864386\n",
            "397 0.00011119700502604246\n",
            "398 0.00010896549065364525\n",
            "399 0.00010649581236066297\n",
            "400 0.00010418848978588358\n",
            "401 0.00010228266182821244\n",
            "402 0.00010000393376685679\n",
            "403 9.80654513114132e-05\n",
            "404 9.607196261640638e-05\n",
            "405 9.430375212104991e-05\n",
            "406 9.272931492887437e-05\n",
            "407 9.073627006728202e-05\n",
            "408 8.890422759577632e-05\n",
            "409 8.731285925023258e-05\n",
            "410 8.528540638508275e-05\n",
            "411 8.35670143715106e-05\n",
            "412 8.220958261517808e-05\n",
            "413 8.090178744168952e-05\n",
            "414 7.957288471516222e-05\n",
            "415 7.781700696796179e-05\n",
            "416 7.624820864293724e-05\n",
            "417 7.496969192288816e-05\n",
            "418 7.388537051156163e-05\n",
            "419 7.240950071718544e-05\n",
            "420 7.103465759428218e-05\n",
            "421 6.972419214434922e-05\n",
            "422 6.853429658804089e-05\n",
            "423 6.776570080546662e-05\n",
            "424 6.645180110353976e-05\n",
            "425 6.529888196382672e-05\n",
            "426 6.440046854550019e-05\n",
            "427 6.36369877611287e-05\n",
            "428 6.254087202250957e-05\n",
            "429 6.149378896225244e-05\n",
            "430 6.0433103499235585e-05\n",
            "431 5.94612502027303e-05\n",
            "432 5.85211455472745e-05\n",
            "433 5.768455594079569e-05\n",
            "434 5.6649205362191424e-05\n",
            "435 5.5733995395712554e-05\n",
            "436 5.5065313063096255e-05\n",
            "437 5.431304089142941e-05\n",
            "438 5.344250166672282e-05\n",
            "439 5.259607860352844e-05\n",
            "440 5.18160049978178e-05\n",
            "441 5.1070273912046105e-05\n",
            "442 5.023930498282425e-05\n",
            "443 4.962975435773842e-05\n",
            "444 4.8643749323673546e-05\n",
            "445 4.786407953361049e-05\n",
            "446 4.72407991765067e-05\n",
            "447 4.657881800085306e-05\n",
            "448 4.6026041673030704e-05\n",
            "449 4.5536424295278266e-05\n",
            "450 4.486368561629206e-05\n",
            "451 4.44359757238999e-05\n",
            "452 4.374197305878624e-05\n",
            "453 4.321350934333168e-05\n",
            "454 4.2702758946688846e-05\n",
            "455 4.208450627629645e-05\n",
            "456 4.1530489397700876e-05\n",
            "457 4.110578083782457e-05\n",
            "458 4.053456359542906e-05\n",
            "459 3.9864589780336246e-05\n",
            "460 3.9316510083153844e-05\n",
            "461 3.904346522176638e-05\n",
            "462 3.849232234642841e-05\n",
            "463 3.779937469516881e-05\n",
            "464 3.742597982636653e-05\n",
            "465 3.703837501234375e-05\n",
            "466 3.650207509053871e-05\n",
            "467 3.6258068575989455e-05\n",
            "468 3.587607352528721e-05\n",
            "469 3.533662675181404e-05\n",
            "470 3.4859600418712944e-05\n",
            "471 3.440092405071482e-05\n",
            "472 3.4220327506773174e-05\n",
            "473 3.365011798450723e-05\n",
            "474 3.345946242916398e-05\n",
            "475 3.311283217044547e-05\n",
            "476 3.2757448934717104e-05\n",
            "477 3.2416690373793244e-05\n",
            "478 3.198976628482342e-05\n",
            "479 3.1803461752133444e-05\n",
            "480 3.1532901630271226e-05\n",
            "481 3.1114665034692734e-05\n",
            "482 3.082282273680903e-05\n",
            "483 3.0456756576313637e-05\n",
            "484 3.0209992473828606e-05\n",
            "485 2.9830256607965566e-05\n",
            "486 2.9482791433110833e-05\n",
            "487 2.9135328077245504e-05\n",
            "488 2.883102570194751e-05\n",
            "489 2.84311445284402e-05\n",
            "490 2.8162572561996058e-05\n",
            "491 2.793457497318741e-05\n",
            "492 2.7653437427943572e-05\n",
            "493 2.7474363378132693e-05\n",
            "494 2.7143714760313742e-05\n",
            "495 2.6799829356605187e-05\n",
            "496 2.668643355718814e-05\n",
            "497 2.6497409635339864e-05\n",
            "498 2.6130406695301645e-05\n",
            "499 2.5788565835682675e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in 2.0*(y_pred - y)? Ans: \n",
        "* It does serve the purpose, but we can run the code without it as well, and the model will get trained, though mathematically we would not be accurate in calling it proper gradient. (without \"2\" convergence is slower, loss is still higher)\n",
        "\n",
        "* We are calculating the gradient of y which is derivate of (y_pred - y)^2. When we perform derivative of a squared entity, \"2\" comes as a multiplier.\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify? Ans: This operation refers to the derivative of ReLU function\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? Ans: 500\n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. Ans: False \n",
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. Ans: True "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnzG31ut_pG1",
        "colab_type": "text"
      },
      "source": [
        "## Quiz:\n",
        "1. (Mostly) whenever we see kernel visualizations online (or some other reference) we are actually seeing:\n",
        " \n",
        " What kernels extract.\n",
        "\n",
        "2. What all do we need to consider when we decide the number of kernels in our 11x11 receptive field layer?\n",
        " \n",
        " * Expressiveness required.\n",
        " * Inter and intra class variations.\n",
        " * Hardware capacity.\n",
        "\n",
        "3. Select the ones which are true\n",
        "   \n",
        "  *  We use strides sometimes on resource constraint hardware.\n",
        "  * We tend not to use strides as they do not read spatial data evenly, causing checkboard issue.\n",
        "  * When using strides, the channels created after convolutions are blurry (not consistent).\n",
        "\n",
        "4. What are the benefits of 1x1 Convolution?\n",
        " \n",
        " * Lesser computation requirement for reducing the number of channels.\n",
        " * Use of existing channels to create complex channels (instead of re-convolution).\n",
        " * Less number of parameters.\n",
        " * Reduces the burden of channel selection on 3x3. \n",
        "\n",
        "5. Why do we not use 1x1 to increase the number of channels? \n",
        " \n",
        "  That's not true. We can use 1x1 to increase the number of channels, just that we need to have a purpose.\n",
        "\n",
        "6. Why do we need an activation function?\n",
        "\n",
        " * To  provide decision making power to the neurons/DNN.\n",
        " * To provide non-linearity.\n",
        "\n",
        "7. Why do we need non-linearity in our neural networks?\n",
        " \n",
        " * Not everything can be expressed using linear functions.\n",
        " * Non-linearity allows DNN to act like a Universal Approximation Function.\n",
        "\n",
        "8. Why sigmoid activation functions are not used? \n",
        " \n",
        " * They cause vanishing gradient issue.\n",
        "\n",
        "9. Select which activation function you'll use in CNN.\n",
        " \n",
        " ReLU\n",
        "\n",
        "10. You'll try and never use Fully Connected Layers?\n",
        " \n",
        " Yes.\n",
        "\n",
        "11. Why do we generally not prefer to add stride of more than 1?\n",
        " \n",
        " It causes checkerboard issue, as we are not reading all pixels equal number of time (ignoring the corner pixels)\n",
        "\n",
        "12. What all features does ReLU provide us?\n",
        " \n",
        " * Easy way to communicate with BackProp to use negative values if that information needs to be filtered out.\n",
        " * Easy way to communicate with BackProp to use positive values if some information needs to be not filtered out.\n",
        " * Very low computation requirements.\n",
        "\n",
        "13. ReLU is defined as:\n",
        "\n",
        " 0 when x is less than or equal to zero\n",
        "\n",
        " x when x is more than zero\n",
        "\n",
        " Any activation function must be differentiable if we were to use it in our DNNs (else backprop would not work). Knowing that we indeed use ReLU, what do you think is the derivative of ReLU?\n",
        "  \n",
        "  0 when x is less than or equal to zero, 1 when x is positive.\n",
        "\n",
        "14. We know that when we use a kernel of size 3x3 and a stride of 1, the receptive field increases by 2. \n",
        "\n",
        " If we use MaxPooling with kernel size 3x3 and with a stride of 1, will the receptive field increase by 2? \n",
        "\n",
        " True.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH7Yp7h9F_c7",
        "colab_type": "text"
      },
      "source": [
        "### That's all Folks!"
      ]
    }
  ]
}