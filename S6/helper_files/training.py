# -*- coding: utf-8 -*-
"""training.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aV25rD3HZWtYAmF3ELtaYtwN2Nx_6cU2
"""

# Imports
from tqdm import tqdm
import torch.nn.functional as F
from regularization import weight_decay

def train(model, device, train_loader, optimizer, epoch, train_losses, train_accuracies, regularizer = 'WO_L1_L2', lamda1 = 0, lamda2 = 0):
  '''
  Function to train the defined model on the training set.

  :param model: (class), defined CNN model
  :param device: (class), either CUDA or CPU
  :param train_loader: (class), dataloader for the training data
  :param optimizer: (class), optimizer for the training
  :param epoch: (int), specific epoch number
  :param train_losses: (list), list of losses accumulated per epoch during training
  :param train_accuracies: (list), list of accuracies accumulated per epoch during training
  :param regularizer: (string), specific regularization option
  :param lamda1: (float), regularization coefficient for L1 loss
  :param lamda2: (float), loss calculated after adding regularization
  '''
  model.train()
  pbar = tqdm(train_loader)
  running_loss = 0.0
  correct = 0
  processed = 0

  for batch_idx, (data, target) in enumerate(pbar):
    # Get samples
    data, target = data.to(device), target.to(device)

    # Init
    optimizer.zero_grad()
    # In PyTorch, we need to set the gradients to zero before  
    # backpropragation because PyTorch accumulates the gradients on subsequent backward passes. 
    # Because of this, when you start your training loop, ideally you should 
    # zero out the gradients so that you do the parameter update correctly.

    # Forward pass
    output = model(data)

    # Calculate loss
    loss = F.nll_loss(output, target)

    # Regularize loss
    loss = weight_decay(model, loss, regularizer, lamda1, lamda2) 
    running_loss += loss.item()

    # Backpropagation
    loss.backward()
    optimizer.step()

    # Update pbar-tqdm
    pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
    correct += pred.eq(target.view_as(pred)).sum().item()
    processed += len(data)
    pbar.set_description(desc= f'Epoch : {epoch}, Loss : {loss.item():.4f}, Batch_id : {batch_idx}, Accuracy : {100*correct/processed:.2f}')
  
  # Train loss and accuracy
  train_losses.append(running_loss/len(train_loader))

  train_accuracies.append(100. * correct / processed)